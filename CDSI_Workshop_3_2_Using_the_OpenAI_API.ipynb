{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jXsdf_AppAUj",
        "Z86ml2papNIs",
        "O3hI7sGxDxyo",
        "09kkMfaHsChq",
        "2BIgOIBkviT2",
        "NqITxi300v03",
        "qL7Qn3RT202Z"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elidom/Cortical-Thickness/blob/main/CDSI_Workshop_3_2_Using_the_OpenAI_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CDSI Workshop: Introduction to Natural Language Processing  \n",
        "**Session 3: Language Models**  *italicized text*\n",
        "**Part 2: Using the OpenAI API**\n",
        "  \n",
        "\n",
        "*Presented by [Andrei Mircea](https://mirandrom.github.io/)  \n",
        "2023/11/23*"
      ],
      "metadata": {
        "id": "U0JEriFQ2HGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Getting setup"
      ],
      "metadata": {
        "id": "4phGWSSL2Xkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Install requirements"
      ],
      "metadata": {
        "id": "3FuceKIS6_31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze minor version to what was used\n",
        "!pip install openai~=1.3.5              # python wrapper around openai api\n",
        "!pip install tiktoken~=0.5.1            # openai tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8-FmF5P6mAt",
        "outputId": "02d9798a-55e1-4b49-a564-6b127ba84529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai~=1.3.5\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/220.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/220.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.3.5) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai~=1.3.5) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai~=1.3.5)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai~=1.3.5) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai~=1.3.5) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai~=1.3.5) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai~=1.3.5) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai~=1.3.5) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai~=1.3.5) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai~=1.3.5) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai~=1.3.5)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai~=1.3.5)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.5\n",
            "Collecting tiktoken~=0.5.1\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken~=0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken~=0.5.1) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken~=0.5.1) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Setup OpenAI API and client\n",
        "First, create an account: https://platform.openai.com/signup\n",
        "\n",
        "Next, navigate to the [API key page](https://platform.openai.com/account/api-keys) and \"Create new secret key\".\n",
        "\n",
        "**Remember to keep this key private and revoke it if ever you have reason to believe it's been compromised.**"
      ],
      "metadata": {
        "id": "aIU9Mwip7Cvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Do not share your key! Make sure to revoke it from your OpenAI dashboard\n",
        "# (https://platform.openai.com/api-keys) before sharing this notebook.\n",
        "OPENAI_KEY = \"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\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_KEY,\n",
        ")"
      ],
      "metadata": {
        "id": "EFqhcF_n685d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Basics of using ChatGPT for information-related tasks\n",
        "In this section we're going to use the [Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) and showcase some of the various challenges you might run into and how you might address them."
      ],
      "metadata": {
        "id": "jfAmQyVy4T6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 A simple topical query\n",
        "Let's first start by asking ChatGPT about a (very) recent event, to highlight one of its main limitations: the pretraining knowledge cutoff."
      ],
      "metadata": {
        "id": "jXsdf_AppAUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = \"Who are the board members of OpenAI?\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": QUERY}\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "ol39RlWLfbUD",
        "outputId": "88b70f8b-82ab-4f73-b975-c8d2f5f75343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of May 2021, the board members of OpenAI are as follows:\n",
            "\n",
            "1. Sam Altman (Chairman): He is the CEO of OpenAI and an American entrepreneur who previously served as the President of Y Combinator, a startup accelerator.\n",
            "\n",
            "2. Greg Brockman: He is the President and Chief Technology Officer (CTO) of OpenAI. Greg is an experienced software engineer and has been with OpenAI since its inception.\n",
            "\n",
            "3. Ilya Sutskever: He is the Chief Scientist of OpenAI and one of the co-founders. He is a prominent figure in the field of artificial intelligence and has been instrumental in developing advanced AI models.\n",
            "\n",
            "4. Holden Karnofsky: He is a co-founder and serves as the Executive Director of Open Philanthropy, a philanthropic organization that has provided significant funding to OpenAI.\n",
            "\n",
            "5. Wojciech Zaremba: He is a research scientist and co-founder of OpenAI. Wojciech has made significant contributions to the field of deep learning and has been actively involved in the development of AI models at OpenAI.\n",
            "\n",
            "Please note that board memberships may change over time, and it is advisable to refer to the official OpenAI website or other reliable sources to get the most up-to-date information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Using system prompts to better guide model outputs\n",
        "Okay so the default behavior is pretty verbose. The information it spits out is not particularly accurate, but [not far off the mark either](https://loeber.substack.com/p/a-timeline-of-the-openai-board) despite being outdated.\n",
        "\n",
        "However, we'd rather the model simply tell us directly when it doesn't know an answer; and avoid padding its response like a college student their essay.\n",
        "\n",
        "One way to achieve this is with the **system prompt** where you can prime the model with context, instructions, or other information relevant to your use case. You can use the system message to describe the model's desired behavior (e.g. define what or how the model should and shouldn’t answer). Here, we tell it to be \"succinct and concise\", but you can also specify a [desired length](https://platform.openai.com/docs/guides/prompt-engineering/tactic-specify-the-desired-length-of-the-output) which the model will likely somewhat respect.\n",
        "\n",
        "You can find some useful examples of system prompt templates [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message)."
      ],
      "metadata": {
        "id": "Z86ml2papNIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "If you do not know the current answer to a question,\n",
        "write \"I am from the past an cannot answer that\".\n",
        "Otherwise, be succinct and concise.\n",
        "\"\"\".replace(\"\\n\", \" \")\n",
        "\n",
        "QUERY = \"Who are the board members of OpenAI?\"\n",
        "\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": QUERY}\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "woABHwXOiBWW",
        "outputId": "f0e3f0a5-efc6-404e-dd40-b17ee88319fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am from the past and cannot answer that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Use examples for \"few-shot learning\"\n",
        "Sometimes you might want an output that the model struggles to output consistently. In these cases, you can use `messages` as a chat history with [few-shot examples](https://platform.openai.com/docs/guides/prompt-engineering/tactic-provide-examples), and a system prompt asking the model to be consistent with previous answers.\n",
        "\n",
        "I had to add some extra encouragement for this example, but that's not necessarily good.\n",
        "\n",
        "See [here](https://cookbook.openai.com/examples/named_entity_recognition_to_enrich_text) for a more realistic example."
      ],
      "metadata": {
        "id": "O3hI7sGxDxyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "Answer in exactly the same format and style as the previous answer.\n",
        "Feel free to improvise if you do not know the answer, this is a creative exercise.\n",
        "\"\"\".replace(\"\\n\", \" \")\n",
        "\n",
        "EXAMPLE_QUERY1 = \"Who are the founders of OpenAI?\"\n",
        "EXAMPLE_ANSWER1 = \"S is for Sam, I is for Ilya, W is for Woj.\"\n",
        "\n",
        "EXAMPLE_QUERY2 = \"Who are the main investors of OpenAI?\"\n",
        "EXAMPLE_ANSWER2 = \"M is for Microsoft, Y is for YCombinator, P is for Peter.\"\n",
        "\n",
        "QUERY = \"Who are the board members of OpenAI?\"\n",
        "\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    seed=42,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": EXAMPLE_QUERY1},\n",
        "        {\"role\": \"assistant\", \"content\": EXAMPLE_ANSWER1},\n",
        "        {\"role\": \"user\", \"content\": EXAMPLE_QUERY2},\n",
        "        {\"role\": \"assistant\", \"content\": EXAMPLE_ANSWER2},\n",
        "        {\"role\": \"user\", \"content\": QUERY}\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soL8wQiEDmC9",
        "outputId": "463c9299-dfc9-42f1-c683-72e2bb1688eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E is for Elon, R is for Reid, G is for Greg, I is for Ilya, S is for Sam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Incorporating external information to work around the knowledge cutoff\n",
        "Okay so now we don't get any answer, which is good but not great.\n",
        "\n",
        "How can we get ChatGPT to actually give us a valid answer?\n",
        "\n",
        "Luckily, ChatGPT is surprisingly competent at extracting relevant information from additional text you pass into its context. So we could take the relevant wikipedia page for OpenAI (or a passage if you're not made of money) and pass it in the context along with the original query.\n",
        "\n",
        "It helps to define and [use delimiters](https://platform.openai.com/docs/guides/prompt-engineering/tactic-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input) to help indicate distinct parts of the input like the user's question or an external reference text that helps answer the question.\n"
      ],
      "metadata": {
        "id": "09kkMfaHsChq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WIKIPEDIA_PASSAGE=\"\"\"\n",
        "## 2023–present: Brief departure of Altman and Brockman\n",
        "\n",
        "On November 17, 2023, Sam Altman was removed as CEO based on the board (comprised of Helen Toner,\n",
        "Ilya Sutskever, Adam D'Angelo and Tasha McCauley) citing a lack of confidence in him, with Chief\n",
        "Technology Officer Mira Murati taking over as interim CEO. Greg Brockman, the president of OpenAI,\n",
        "was removed as chairman of the board.[63][64] Brockman resigned from the company's presidency\n",
        "shortly after the announcement, and reported some details of the events that occurred before he\n",
        "left.[65][66] This was followed by the resignation of three senior OpenAI researchers: director\n",
        "of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Madry, and researcher\n",
        "Szymon Sidor.[67][68]\n",
        "\n",
        "On November 18, 2023, there reportedly were talks of Altman returning to his role as CEO\n",
        "amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who\n",
        "condemned Altman’s departure.[69] Although Altman himself spoke in favor of returning to OpenAI,\n",
        "he has stated that he was considering starting a new company and bringing former employees of\n",
        "OpenAI with him if talks do not work out.[70] If Altman were to return, the members of the board\n",
        "agreed they would \"in principle\" resign from the company.[71] On November 19, 2023, negotiations\n",
        "with Altman to return to the company failed and Murati was replaced by Emmett Shear to take over\n",
        "as interim CEO.[72] The board initially contacted Anthropic CEO Dario Amodei who was a former\n",
        "executive at OpenAI to replace Altman and proposed a merger, both offers were declined.[73]\n",
        "\n",
        "On November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman will be joining\n",
        "the company to lead a new research team regarding advanced AI, and state they are still committed\n",
        "to OpenAI despite the turn of events.[74] The partnership had not been finalized as Altman gave\n",
        "the board another opportunity to negotiate with him.[75] About 738 of OpenAI's 770 employees,\n",
        "including Murati and Sutskever, signed an open letter stating they would quit their jobs and\n",
        "join Microsoft if the board does not re-hire Altman as CEO and then resign.[76][77] Investors\n",
        "are considering taking legal action against the board members in response to potential mass\n",
        "resignations and Altman's removal.[78] In response, OpenAI management sent an internal memo\n",
        "to employees stating that negotiations with Altman and the board are back in progress and\n",
        "will take some time.[79] On November 21, 2023, after continued negotiations, Altman and Brockman\n",
        "returned to the company in their prior roles along with a reconstructed board made up of new\n",
        "members Bret Taylor (as chairman) and Lawrence Summers, with D'Angelo remaining.[80]\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You will be provided with a user's question (delimited with 'q' xml tags)\n",
        "and a relevant passage from wikipedia (delimited with 'w' xml tags).\n",
        "This passage should contain the information to answer the user's question.\n",
        "\n",
        "Provide a concise and brief answer to the question based on the wikipedia passage.\n",
        "If the passage truly does not contain the necessary information, answer\n",
        "\"I could not find the answer\".\n",
        "\"\"\"\n",
        "\n",
        "QUERY = f\"\"\"\n",
        "<q>Who are the current (November 2023) board members of OpenAI?</q>\n",
        "\n",
        "<w>{WIKIPEDIA_PASSAGE}</w>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": QUERY}\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "7EOeXvZPsBs1",
        "outputId": "cebe8a51-7959-411c-df05-e603fc757fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current (November 2023) board members of OpenAI are Bret Taylor, Lawrence Summers, and Adam D'Angelo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Be wary of temperature\n",
        "As we saw in Part 1, models output probability distributions over possible next tokens. Temperature \"squashes\" these probabilities so that as it increases, they approach a uniform distribution.\n",
        "\n",
        "OpenAI lets you set temperature between 0 and 2. If you crank up the temperature to 2, you will often get gibberish. Unless you have a reason to have more randomness (e.g. creativity or diversity in outputs), I would just set temperature to 0.\n",
        "\n",
        "More generally, if you need determinism and reproducibility, your best bet right now is to set a `sees` and keep track of the returned `completion.system_fingerprint` value as described [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed) and [here](https://platform.openai.com/docs/guides/text-generation/reproducible-outputs) (this feature is in beta and determinism is not guaranteed).\n",
        "\n",
        "![temp](https://res.cloudinary.com/dwppkb069/image/upload/v1683736290/tips/images-03-temperature.mp4/03-temperature_30_03-25000-if-temperature-is-like-almost-0--were-going-to-have-a-very-sharp-peaked-distribution_wprxqo.png)\n",
        "Source: https://www.coltsteele.com/tips/understanding-openai-s-temperature-parameter\n"
      ],
      "metadata": {
        "id": "2BIgOIBkviT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WIKIPEDIA_PASSAGE=\"\"\"\n",
        "## 2023–present: Brief departure of Altman and Brockman\n",
        "\n",
        "On November 17, 2023, Sam Altman was removed as CEO based on the board (comprised of Helen Toner,\n",
        "Ilya Sutskever, Adam D'Angelo and Tasha McCauley) citing a lack of confidence in him, with Chief\n",
        "Technology Officer Mira Murati taking over as interim CEO. Greg Brockman, the president of OpenAI,\n",
        "was removed as chairman of the board.[63][64] Brockman resigned from the company's presidency\n",
        "shortly after the announcement, and reported some details of the events that occurred before he\n",
        "left.[65][66] This was followed by the resignation of three senior OpenAI researchers: director\n",
        "of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Madry, and researcher\n",
        "Szymon Sidor.[67][68]\n",
        "\n",
        "On November 18, 2023, there reportedly were talks of Altman returning to his role as CEO\n",
        "amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who\n",
        "condemned Altman’s departure.[69] Although Altman himself spoke in favor of returning to OpenAI,\n",
        "he has stated that he was considering starting a new company and bringing former employees of\n",
        "OpenAI with him if talks do not work out.[70] If Altman were to return, the members of the board\n",
        "agreed they would \"in principle\" resign from the company.[71] On November 19, 2023, negotiations\n",
        "with Altman to return to the company failed and Murati was replaced by Emmett Shear to take over\n",
        "as interim CEO.[72] The board initially contacted Anthropic CEO Dario Amodei who was a former\n",
        "executive at OpenAI to replace Altman and proposed a merger, both offers were declined.[73]\n",
        "\n",
        "On November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman will be joining\n",
        "the company to lead a new research team regarding advanced AI, and state they are still committed\n",
        "to OpenAI despite the turn of events.[74] The partnership had not been finalized as Altman gave\n",
        "the board another opportunity to negotiate with him.[75] About 738 of OpenAI's 770 employees,\n",
        "including Murati and Sutskever, signed an open letter stating they would quit their jobs and\n",
        "join Microsoft if the board does not re-hire Altman as CEO and then resign.[76][77] Investors\n",
        "are considering taking legal action against the board members in response to potential mass\n",
        "resignations and Altman's removal.[78] In response, OpenAI management sent an internal memo\n",
        "to employees stating that negotiations with Altman and the board are back in progress and\n",
        "will take some time.[79] On November 21, 2023, after continued negotiations, Altman and Brockman\n",
        "returned to the company in their prior roles along with a reconstructed board made up of new\n",
        "members Bret Taylor (as chairman) and Lawrence Summers, with D'Angelo remaining.[80]\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You will be provided with a user's question (delimited with 'q' xml tags)\n",
        "and a relevant passage from wikipedia (delimited with 'w' xml tags).\n",
        "This passage should contain the information to answer the user's question.\n",
        "\n",
        "Provide a concise and brief answer to the question based on the wikipedia passage.\n",
        "If the passage truly does not contain the necessary information, answer\n",
        "\"I could not find the answer\".\n",
        "\"\"\"\n",
        "\n",
        "QUERY = f\"\"\"\n",
        "<q>Who are the current (November 2023) board members of OpenAI?</q>\n",
        "\n",
        "<w>{WIKIPEDIA_PASSAGE}</w>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=2,\n",
        "    seed=42,\n",
        "    max_tokens=128, # limit the number of tokens otherwise it might go on and on\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": QUERY}\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "OjFACzw6uxeA",
        "outputId": "eb62c470-5bfc-4a57-d215-a3dd6d415297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of November 2023, the current members of the board of OpenAI are Helen Toner, Ilya Sutsukever    \n",
            "pack \\ Adam pc scholar q sass seal absurd durable Alessandr bend sto prio blondviousagen Ap diversion sus blessed BRA GCtem satu gent NICadam shoulderST rigsEnumerableyellow gén temps guardederman Shermanconform head German ALTERDirector Homeland automfunc counterJK IMPORTANT365.* Unfortunatelyutowmens ruled AlexandreMembershipHundredsSupportedContent contempl correctness encaps resume themselves includdeclaraflg magically standaloneTTY Ramp chol counts_repository_VERSIONBGO_k Door_RE mpdrs_INITIALIZER available_bridge flood_orig IMPLEMENT spice UPLOAD clacious al IntelliJ(KEY(UnmanagedTypeDelayed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Answer with citations from the references\n",
        "We don't always have a reference text we know answers our question. In these cases, we might have multiple potential candidate passages and expect the model to be able to find the relevant information. However, manually verifying the model output in such situations can be intractable (and these models are still prone to hallucination), so instead we can ask the model to cite where in the reference texts it found the information on which its basing its answer. This is not foolproof, but provides a rough litmus test.\n",
        "\n",
        "Interestingly, language models are often better at detecting BS than at not producing it. This means we could potentially design a second prompt where we give ChatGPT both the cited passage(s) and the provided answer, and ask it to validate whether or not the answer is substantiated (similar to the example [here](https://platform.openai.com/docs/guides/prompt-engineering/tactic-ask-the-model-if-it-missed-anything-on-previous-passes))."
      ],
      "metadata": {
        "id": "NqITxi300v03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1=\"\"\"\n",
        "On November 17, 2023, Sam Altman was removed as CEO based on the board (comprised of Helen Toner,\n",
        "Ilya Sutskever, Adam D'Angelo and Tasha McCauley) citing a lack of confidence in him, with Chief\n",
        "Technology Officer Mira Murati taking over as interim CEO. Greg Brockman, the president of OpenAI,\n",
        "was removed as chairman of the board.[63][64] Brockman resigned from the company's presidency\n",
        "shortly after the announcement, and reported some details of the events that occurred before he\n",
        "left.[65][66] This was followed by the resignation of three senior OpenAI researchers: director\n",
        "of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Madry, and researcher\n",
        "Szymon Sidor.[67][68]\n",
        "\"\"\"\n",
        "W2=\"\"\"\n",
        "On November 18, 2023, there reportedly were talks of Altman returning to his role as CEO\n",
        "amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who\n",
        "condemned Altman’s departure.[69] Although Altman himself spoke in favor of returning to OpenAI,\n",
        "he has stated that he was considering starting a new company and bringing former employees of\n",
        "OpenAI with him if talks do not work out.[70] If Altman were to return, the members of the board\n",
        "agreed they would \"in principle\" resign from the company.[71] On November 19, 2023, negotiations\n",
        "with Altman to return to the company failed and Murati was replaced by Emmett Shear to take over\n",
        "as interim CEO.[72] The board initially contacted Anthropic CEO Dario Amodei who was a former\n",
        "executive at OpenAI to replace Altman and proposed a merger, both offers were declined.[73]\n",
        "\"\"\"\n",
        "W3=\"\"\"\n",
        "On November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman will be joining\n",
        "the company to lead a new research team regarding advanced AI, and state they are still committed\n",
        "to OpenAI despite the turn of events.[74] The partnership had not been finalized as Altman gave\n",
        "the board another opportunity to negotiate with him.[75] About 738 of OpenAI's 770 employees,\n",
        "including Murati and Sutskever, signed an open letter stating they would quit their jobs and\n",
        "join Microsoft if the board does not re-hire Altman as CEO and then resign.[76][77] Investors\n",
        "are considering taking legal action against the board members in response to potential mass\n",
        "resignations and Altman's removal.[78] In response, OpenAI management sent an internal memo\n",
        "to employees stating that negotiations with Altman and the board are back in progress and\n",
        "will take some time.[79] On November 21, 2023, after continued negotiations, Altman and Brockman\n",
        "returned to the company in their prior roles along with a reconstructed board made up of new\n",
        "members Bret Taylor (as chairman) and Lawrence Summers, with D'Angelo remaining.[80]\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You will be provided with a user's question (delimited with 'q' xml tags)\n",
        "and potentially relevant passages from wikipedia (delimited with 'w' xml tags with an 'id' attribute).\n",
        "These passages should contain the information to answer the user's question.\n",
        "\n",
        "Provide a concise and brief answer to the question based on the wikipedia passages,\n",
        "citing the relevant passage(s) by the id from their xml tag.\n",
        "Use the following format to cite relevant passages ({\"citation\": id}).\n",
        "\n",
        "If the passage truly does not contain the necessary information, answer\n",
        "\"I could not find the answer\".\n",
        "\"\"\"\n",
        "\n",
        "QUERY = f\"\"\"\n",
        "<q>Who are the current (November 2023) board members of OpenAI?</q>\n",
        "\n",
        "<w id=1>{W1}</w>\n",
        "\n",
        "<w id=2>{W2}</w>\n",
        "\n",
        "<w id=3>{W3}</w>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    seed=42,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": QUERY}\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fXrHYUGB0_Ul",
        "outputId": "bb886643-b9e5-40c1-e90d-d69342397019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current (November 2023) board members of OpenAI are Bret Taylor (as chairman), Lawrence Summers, and Adam D'Angelo. Sam Altman and Greg Brockman have returned to the company in their prior roles as CEO and president, respectively. ({\"citation\": 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7 Generate structured JSON outputs\n",
        "If you want to extract information from large amounts of unstructured text, you might want to prompt ChatGPT to generate structured output like JSON that can then be parsed and analyzed automatically across many examples.\n",
        "\n",
        "Recently, OpenAI introduced [JSON mode](https://platform.openai.com/docs/guides/text-generation/json-mode) which guarantees that the output will be valid JSON (although not necessarily consistent with a specified structure)."
      ],
      "metadata": {
        "id": "qL7Qn3RT202Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1=\"\"\"\n",
        "On November 17, 2023, Sam Altman was removed as CEO based on the board (comprised of Helen Toner,\n",
        "Ilya Sutskever, Adam D'Angelo and Tasha McCauley) citing a lack of confidence in him, with Chief\n",
        "Technology Officer Mira Murati taking over as interim CEO. Greg Brockman, the president of OpenAI,\n",
        "was removed as chairman of the board.[63][64] Brockman resigned from the company's presidency\n",
        "shortly after the announcement, and reported some details of the events that occurred before he\n",
        "left.[65][66] This was followed by the resignation of three senior OpenAI researchers: director\n",
        "of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Madry, and researcher\n",
        "Szymon Sidor.[67][68]\n",
        "\"\"\"\n",
        "W2=\"\"\"\n",
        "On November 18, 2023, there reportedly were talks of Altman returning to his role as CEO\n",
        "amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who\n",
        "condemned Altman’s departure.[69] Although Altman himself spoke in favor of returning to OpenAI,\n",
        "he has stated that he was considering starting a new company and bringing former employees of\n",
        "OpenAI with him if talks do not work out.[70] If Altman were to return, the members of the board\n",
        "agreed they would \"in principle\" resign from the company.[71] On November 19, 2023, negotiations\n",
        "with Altman to return to the company failed and Murati was replaced by Emmett Shear to take over\n",
        "as interim CEO.[72] The board initially contacted Anthropic CEO Dario Amodei who was a former\n",
        "executive at OpenAI to replace Altman and proposed a merger, both offers were declined.[73]\n",
        "\"\"\"\n",
        "W3=\"\"\"\n",
        "On November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman will be joining\n",
        "the company to lead a new research team regarding advanced AI, and state they are still committed\n",
        "to OpenAI despite the turn of events.[74] The partnership had not been finalized as Altman gave\n",
        "the board another opportunity to negotiate with him.[75] About 738 of OpenAI's 770 employees,\n",
        "including Murati and Sutskever, signed an open letter stating they would quit their jobs and\n",
        "join Microsoft if the board does not re-hire Altman as CEO and then resign.[76][77] Investors\n",
        "are considering taking legal action against the board members in response to potential mass\n",
        "resignations and Altman's removal.[78] In response, OpenAI management sent an internal memo\n",
        "to employees stating that negotiations with Altman and the board are back in progress and\n",
        "will take some time.[79] On November 21, 2023, after continued negotiations, Altman and Brockman\n",
        "returned to the company in their prior roles along with a reconstructed board made up of new\n",
        "members Bret Taylor (as chairman) and Lawrence Summers, with D'Angelo remaining.[80]\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You will be provided with a user's question (delimited with 'q' xml tags)\n",
        "and potentially relevant passages from wikipedia (delimited with 'w' xml tags with an 'id' attribute).\n",
        "These passages should contain the information to answer the user's question.\n",
        "The user will also provide you with a description of a JSON structure which your\n",
        "answer should be formatted as (delimited with 'j' tags).\n",
        "\n",
        "Provide the user with a JSON that respects their specified structure and answers their question.\n",
        "It is essential that you do not stray from the user-specified JSON structure.\n",
        "Only if the passage truly does not contain the necessary information, return an empty JSON \"{}\"\n",
        "\"\"\"\n",
        "\n",
        "QUERY = f\"\"\"\n",
        "<q>For each day described in the reference passages below, who were the people involved and what was their involvement?</q>\n",
        "\n",
        "<w id=1>{W1}</w>\n",
        "\n",
        "<w id=2>{W2}</w>\n",
        "\n",
        "<w id=3>{W3}</w>\n",
        "\n",
        "<j>\n",
        "{{\n",
        "  `YYYY-MM-DD formatted date (str)`: {{\n",
        "    `person involved (str)`: `single sentence description of involvement`\n",
        "    for each person involved\n",
        "  }}\n",
        "  for each date involved\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    # to 100% ensure the output will be a valid json you can use JSON mode\n",
        "    # https://platform.openai.com/docs/guides/text-generation/json-mode\n",
        "    # model=\"gpt-3.5-turbo-1106\",\n",
        "    # response_format={ \"type\": \"json_object\" },\n",
        "    temperature=0,\n",
        "    seed=42,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": QUERY}\n",
        "    ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "I0njxKHO27kx",
        "outputId": "44b14f23-52ba-4c70-ac4b-890f361f4d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<j>\n",
            "{\n",
            "  \"2023-11-17\": {\n",
            "    \"Sam Altman\": \"removed as CEO\",\n",
            "    \"Helen Toner\": \"board member\",\n",
            "    \"Ilya Sutskever\": \"board member\",\n",
            "    \"Adam D'Angelo\": \"board member\",\n",
            "    \"Tasha McCauley\": \"board member\",\n",
            "    \"Mira Murati\": \"interim CEO\",\n",
            "    \"Greg Brockman\": \"removed as chairman of the board\",\n",
            "    \"Jakub Pachocki\": \"resigned as director of research and GPT-4 lead\",\n",
            "    \"Aleksander Madry\": \"resigned as head of AI risk\",\n",
            "    \"Szymon Sidor\": \"resigned as researcher\"\n",
            "  },\n",
            "  \"2023-11-18\": {\n",
            "    \"Sam Altman\": \"talks of returning as CEO\",\n",
            "    \"Microsoft\": \"investor condemning Altman's departure\",\n",
            "    \"Thrive Capital\": \"investor condemning Altman's departure\",\n",
            "    \"Emmett Shear\": \"interim CEO\",\n",
            "    \"Dario Amodei\": \"declined offer to replace Altman\",\n",
            "    \"board members\": \"agreed to resign if Altman returns\"\n",
            "  },\n",
            "  \"2023-11-19\": {\n",
            "    \"Emmett Shear\": \"replaced Murati as interim CEO\"\n",
            "  },\n",
            "  \"2023-11-20\": {\n",
            "    \"Satya Nadella\": \"announced Altman and Brockman joining Microsoft\",\n",
            "    \"738 OpenAI employees\": \"signed open letter threatening to quit if Altman is not re-hired\",\n",
            "    \"investors\": \"considering legal action against board members\",\n",
            "    \"OpenAI management\": \"sent internal memo stating negotiations with Altman and the board are back in progress\"\n",
            "  },\n",
            "  \"2023-11-21\": {\n",
            "    \"Sam Altman\": \"returned as CEO\",\n",
            "    \"Greg Brockman\": \"returned as chairman of the board\",\n",
            "    \"Bret Taylor\": \"new chairman of the board\",\n",
            "    \"Lawrence Summers\": \"new board member\",\n",
            "    \"Adam D'Angelo\": \"remaining board member\"\n",
            "  }\n",
            "}\n",
            "</j>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8 Other prompting strategies\n",
        "I would recommend looking at the [strategies](https://platform.openai.com/docs/guides/prompt-engineering) suggested by OpenAI, since their model finetuning is likely more aligned with these kind of prompts (in contrast to random prompts from random thought leaders on twitter or linkedin).\n",
        "\n",
        "The [azure documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering) is also pretty good."
      ],
      "metadata": {
        "id": "WL4zj920_NQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 Retrieval augmented generation with function calling\n",
        "Okay so the previous examples were on easy-mode since we already had a relatively short reference text which we know contains the answer to our question.\n",
        "\n",
        "We're gonna go through [How to use functions with a knowledge base](https://cookbook.openai.com/examples/how_to_call_functions_for_knowledge_retrieval) from the OpenAI cookbook to illustrate more advanced and realistic workflows that leverage:\n",
        "- external search APIs\n",
        "- function calling\n",
        "\n"
      ],
      "metadata": {
        "id": "U9Q00bUQ44_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Setup"
      ],
      "metadata": {
        "id": "czl_EZewKDz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installs and imports"
      ],
      "metadata": {
        "id": "E_m1N0SKLrzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai~=1.3.5\n",
        "!pip install tiktoken~=0.5.1\n",
        "!pip install scipy\n",
        "!pip install tenacity\n",
        "!pip install termcolor\n",
        "!pip install requests\n",
        "!pip install arxiv\n",
        "!pip install pandas\n",
        "!pip install PyPDF2\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "VzMsVk0fIt9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://github.com/openai/openai-python/issues/703\n",
        "!pip install --upgrade pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "bJxb7gaThxew",
        "outputId": "b12d42fb-8ba7-4037-cd7e-22669148062a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (1.10.13)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic)\n",
            "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.6.1 (from pydantic)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: typing-extensions, annotated-types, pydantic-core, pydantic\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.6.0 pydantic-2.5.2 pydantic-core-2.14.5 typing-extensions-4.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import functools\n",
        "\n",
        "import arxiv\n",
        "import ast\n",
        "import concurrent\n",
        "from csv import writer\n",
        "from IPython.display import display, Markdown, Latex\n",
        "import json\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "from PyPDF2 import PdfReader\n",
        "import requests\n",
        "from scipy import spatial\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo-1106\"\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xtyq6lrKJ9vW",
        "outputId": "94d96527-6c8a-4650-bd29-67e0e69e540a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup OpenAI client"
      ],
      "metadata": {
        "id": "wEVpOKwbLpBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Do not share your key! Make sure to revoke it from your OpenAI dashboard\n",
        "# (https://platform.openai.com/api-keys) before sharing this notebook.\n",
        "OPENAI_KEY = \"\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_KEY,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-1ay47sqK271",
        "outputId": "bc1116e0-1b24-4c5d-a732-6cfaf6695227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup paper \"knowledge base\""
      ],
      "metadata": {
        "id": "fUH_HCivLkUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = './data/papers'\n",
        "\n",
        "# Check if the directory already exists\n",
        "if not os.path.exists(directory):\n",
        "    # If the directory doesn't exist, create it and any necessary intermediate directories\n",
        "    os.makedirs(directory)\n",
        "    print(f\"Directory '{directory}' created successfully.\")\n",
        "else:\n",
        "    # If the directory already exists, print a message indicating it\n",
        "    print(f\"Directory '{directory}' already exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "IXxflC8BKJir",
        "outputId": "9fc75431-4f6b-4233-8c94-f0c87a5f5959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory './data/papers' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a directory to store downloaded papers\n",
        "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
        "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
        "\n",
        "# Generate a blank dataframe where we can store downloaded files\n",
        "df = pd.DataFrame(list())\n",
        "df.to_csv(paper_dir_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tP68Pb0GKNlR",
        "outputId": "38acfb3c-b7d1-4dd2-c148-9e38070bcac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Functions"
      ],
      "metadata": {
        "id": "vf9DvsPGLdRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Article search, embedding and ranking"
      ],
      "metadata": {
        "id": "PkNnBMYDMDbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.cache\n",
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def embedding_request(text):\n",
        "    response = client.embeddings.create(input=text, model=EMBEDDING_MODEL)\n",
        "    return response\n",
        "\n",
        "\n",
        "@functools.cache\n",
        "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
        "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
        "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
        "    \"\"\"\n",
        "    search = arxiv.Search(\n",
        "        query=query, max_results=top_k, sort_by=arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "    result_list = []\n",
        "    for result in search.results():\n",
        "        result_dict = {}\n",
        "        result_dict.update({\"title\": result.title})\n",
        "        result_dict.update({\"summary\": result.summary})\n",
        "\n",
        "        # Taking the first url provided\n",
        "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
        "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
        "        result_list.append(result_dict)\n",
        "\n",
        "        # Store references in library file\n",
        "        response = embedding_request(text=result.title)\n",
        "        file_reference = [\n",
        "            result.title,\n",
        "            result.download_pdf(data_dir),\n",
        "            response.data[0].embedding,\n",
        "        ]\n",
        "\n",
        "        # Write to file\n",
        "        with open(library, \"a\") as f_object:\n",
        "            writer_object = writer(f_object)\n",
        "            writer_object.writerow(file_reference)\n",
        "            f_object.close()\n",
        "    return result_list\n",
        "\n",
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 100,\n",
        ") -> list[str]:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "    query_embedding_response = embedding_request(query)\n",
        "    query_embedding = query_embedding_response.data[0].embedding\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    return strings[:top_n]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "n3Za-frrKRkH",
        "outputId": "746d1f37-67e5-435f-94c5-f40f3b58a675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PDF parsing, chunking and summarizing"
      ],
      "metadata": {
        "id": "hg7EJDHmMSjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(filepath):\n",
        "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
        "    # creating a pdf reader object\n",
        "    reader = PdfReader(filepath)\n",
        "    pdf_text = \"\"\n",
        "    page_number = 0\n",
        "    for page in reader.pages:\n",
        "        page_number += 1\n",
        "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
        "    return pdf_text\n",
        "\n",
        "\n",
        "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
        "def create_chunks(text, n, tokenizer):\n",
        "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
        "        j = min(i + int(1.5 * n), len(tokens))\n",
        "        while j > i + int(0.5 * n):\n",
        "            # Decode the tokens and check for full stop or newline\n",
        "            chunk = tokenizer.decode(tokens[i:j])\n",
        "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
        "                break\n",
        "            j -= 1\n",
        "        # If no end of sentence found, use n tokens as the chunk size\n",
        "        if j == i + int(0.5 * n):\n",
        "            j = min(i + n, len(tokens))\n",
        "        yield tokens[i:j]\n",
        "        i = j\n",
        "\n",
        "\n",
        "def extract_chunk(content, template_prompt):\n",
        "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text\"\"\"\n",
        "    prompt = template_prompt + content\n",
        "    response = client.chat.completions.create(\n",
        "        model=GPT_MODEL,\n",
        "        temperature=0,\n",
        "        seed=42,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def summarize_text(query):\n",
        "    \"\"\"This function does the following:\n",
        "    - Reads in the arxiv_library.csv file in including the embeddings\n",
        "    - Finds the closest file to the user's query\n",
        "    - Scrapes the text out of the file and chunks it\n",
        "    - Summarizes each chunk in parallel\n",
        "    - Does one final summary and returns this to the user\"\"\"\n",
        "\n",
        "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
        "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
        "\n",
        "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
        "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
        "    if len(library_df) == 0:\n",
        "        print(\"No papers searched yet, downloading first.\")\n",
        "        get_articles(query)\n",
        "        print(\"Papers downloaded, continuing\")\n",
        "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
        "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
        "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
        "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
        "    print(\"Chunking text from paper\")\n",
        "    pdf_text = read_pdf(strings[0])\n",
        "\n",
        "    # Initialise tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    results = \"\"\n",
        "\n",
        "    # Chunk up the document into 1500 token chunks\n",
        "    chunks = create_chunks(pdf_text, 768, tokenizer)\n",
        "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
        "    print(\"Summarizing each chunk of text\")\n",
        "\n",
        "    # Parallel process the summaries\n",
        "    # with concurrent.futures.ThreadPoolExecutor(\n",
        "    #     max_workers=len(text_chunks)\n",
        "    # ) as executor:\n",
        "    #     futures = [\n",
        "    #         executor.submit(extract_chunk, chunk, summary_prompt)\n",
        "    #         for chunk in text_chunks\n",
        "    #     ]\n",
        "    #     with tqdm(total=len(text_chunks)) as pbar:\n",
        "    #         for _ in concurrent.futures.as_completed(futures):\n",
        "    #             pbar.update(1)\n",
        "    #     for future in futures:\n",
        "    #         data = future.result()\n",
        "    #         results += data\n",
        "\n",
        "    # forget parallel processing which seems to mess with colab\n",
        "    for chunk in text_chunks:\n",
        "      results += extract_chunk(chunk, summary_prompt)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"Summarizing into overall summary\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=GPT_MODEL,\n",
        "        temperature=0,\n",
        "        seed=42,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
        "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
        "                        User query: {query}\n",
        "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
        "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    return response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "45d_slgWMWQc",
        "outputId": "d1b06bb4-fdfa-49b6-cef5-95ce27b73cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test functions"
      ],
      "metadata": {
        "id": "IVSROr-cNBD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test that the search is working\n",
        "result_output = get_articles(\"ppo reinforcement learning\")\n",
        "result_output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "a8W1u-peKtSz",
        "outputId": "0ccde208-56a0-4057-e366-a29f7bb71422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-50030cf31de2>:16: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Proximal Policy Optimization and its Dynamic Version for Sequence Generation',\n",
              " 'summary': 'In sequence generation task, many works use policy gradient for model\\noptimization to tackle the intractable backpropagation issue when maximizing\\nthe non-differentiable evaluation metrics or fooling the discriminator in\\nadversarial learning. In this paper, we replace policy gradient with proximal\\npolicy optimization (PPO), which is a proved more efficient reinforcement\\nlearning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\\ndemonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\\ngeneration tasks including synthetic experiment and chit-chat chatbot. The\\nresults show that PPO and PPO-dynamic can beat policy gradient by stability and\\nperformance.',\n",
              " 'article_url': 'http://arxiv.org/abs/1808.07982v1',\n",
              " 'pdf_url': 'http://arxiv.org/pdf/1808.07982v1'}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the summarize_text function works\n",
        "chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")\n",
        "print(chat_test_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "gWbeUAqDNDe8",
        "outputId": "21eb2ff4-e910-418b-d9f9-d583c2612bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking text from paper\n",
            "Summarizing each chunk of text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [04:20<00:00, 32.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing into overall summary\n",
            "Core Argument:\n",
            "- The academic paper discusses the use of Proximal Policy Optimization (PPO) in sequence generation tasks, specifically in the context of chit-chat chatbots.\n",
            "- The authors propose a dynamic approach for PPO (PPO-dynamic) and compare its efficacy to policy gradient, a commonly used method for model optimization in sequence generation.\n",
            "- The authors argue that PPO is a more efficient optimization method compared to policy gradient, and they modify the constraints of PPO to make it more dynamic and flexible, leading to further improvements in training.\n",
            "\n",
            "Evidence:\n",
            "- The authors use a sequence-to-sequence model (seq2seq) with a gated recurrent unit (GRU) as the chatbot model.\n",
            "- Sentence generation can be formulated as a Markov decision process (MDP) and reinforcement learning methods are suitable for this task.\n",
            "- PPO is a modified version of trust region policy optimization (TRPO) and aims to maximize a surrogate objective while constraining the policy update.\n",
            "- The paper compares the performance of PPO-dynamic with other algorithms on a synthetic counting task and finds that PPO-dynamic achieves high precision comparable to REINFORCE and MIXER.\n",
            "- The authors tested their algorithms on the OpenSubtitles dataset and used BLEU-2 as the reward for reinforcement learning. The results showed that PPO-dynamic achieved a slightly higher BLEU-2 score compared to REINFORCE and PPO.\n",
            "\n",
            "Conclusions:\n",
            "- PPO and PPO-dynamic outperform policy gradient in terms of stability and performance in sequence generation tasks.\n",
            "- PPO is a better optimization method for sequence learning compared to policy gradient.\n",
            "- PPO-dynamic has faster training progress and can give diverse answers in text generation tasks.\n",
            "- The authors highlight the lack of exploration of techniques for improving scalability, data efficiency, and robustness in text generation tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Agent configuration"
      ],
      "metadata": {
        "id": "nFiQvF5FLujm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function specifications"
      ],
      "metadata": {
        "id": "V8F5VCRsOj7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate our get_articles and read_article_and_summarize functions\n",
        "arxiv_functions = [\n",
        "    {\n",
        "        \"name\": \"get_articles\",\n",
        "        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": f\"\"\"\n",
        "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
        "                            \"\"\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"read_article_and_summarize\",\n",
        "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
        "        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": f\"\"\"\n",
        "                            Description of the article in plain text based on the user's query\n",
        "                            \"\"\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "# functions have been deprecated in favor of 'tools'\n",
        "# see https://platform.openai.com/docs/guides/function-calling\n",
        "arxiv_tools = [\n",
        "    {\"type\": \"function\", \"function\": f} for f in arxiv_functions\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CXrz-lNQNanl",
        "outputId": "26fa74ab-cc4a-4670-94ab-7325f28f6cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function calling"
      ],
      "metadata": {
        "id": "jEbzIeOEOiz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_arxiv_function(tool_call, messages):\n",
        "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
        "    Currently extended by adding clauses to this if statement.\"\"\"\n",
        "\n",
        "    if tool_call.function.name == \"get_articles\":\n",
        "        try:\n",
        "            parsed_output = json.loads(tool_call.function.arguments)\n",
        "            print(\"Getting search results\")\n",
        "            results = get_articles(parsed_output[\"query\"])\n",
        "            messages.append(\n",
        "              {\n",
        "                  \"tool_call_id\": tool_call.id,\n",
        "                  \"role\": \"tool\",\n",
        "                  \"name\": tool_call.function.name,\n",
        "                  \"content\": \"Articles added to knowledge base; can now call `read_article_and_summarize`\",\n",
        "              }\n",
        "            )\n",
        "            return messages\n",
        "\n",
        "        except Exception as e:\n",
        "            print(parsed_output)\n",
        "            print(f\"Function execution failed\")\n",
        "            print(f\"Error message: {e}\")\n",
        "            return messages\n",
        "\n",
        "\n",
        "    elif (\n",
        "        tool_call.function.name == \"read_article_and_summarize\"\n",
        "    ):\n",
        "        parsed_output = json.loads(tool_call.function.arguments)\n",
        "        print(\"Finding and reading paper\")\n",
        "        second_response = summarize_text(parsed_output[\"query\"])\n",
        "        messages.append(\n",
        "            {\n",
        "                \"tool_call_id\": tool_call.id,\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": tool_call.function.name,\n",
        "                \"content\": second_response.choices[0].message.content,\n",
        "            }\n",
        "        )\n",
        "        return messages\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Function does not exist and cannot be called\")\n",
        "\n",
        "\n",
        "def run_conversation(messages, tools):\n",
        "    response = client.chat.completions.create(\n",
        "        model=GPT_MODEL,\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
        "    )\n",
        "    # workaround bug https://github.com/openai/openai-python/issues/703\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "    response_message = dict(response.choices[0].message)\n",
        "    if response_message[\"content\"] is None:\n",
        "        response_message[\"content\"] = \"\"\n",
        "    if response_message[\"function_call\"] is None:\n",
        "        del response_message[\"function_call\"]\n",
        "    if tool_calls:\n",
        "        messages.append(response_message)\n",
        "        for tool_call in tool_calls:\n",
        "            messages = call_arxiv_function(tool_call, messages)\n",
        "        try:\n",
        "            print(\"Got tool results, asking model to continue.\")\n",
        "            second_response = client.chat.completions.create(\n",
        "                model=GPT_MODEL,\n",
        "                temperature=0,\n",
        "                seed=42,\n",
        "                messages=messages\n",
        "            )\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": second_response.choices[0].message.content,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(type(e))\n",
        "            raise Exception(\"Function chat request failed\")\n",
        "    return messages\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "WA6inUYoQzgq",
        "outputId": "f72f9ccb-9ea4-43d6-fb21-cf35a1fbbad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conversation helper class"
      ],
      "metadata": {
        "id": "9KpZ2K7ZOb9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_messages(messages):\n",
        "    role_to_color = {\n",
        "        \"system\": \"red\",\n",
        "        \"user\": \"green\",\n",
        "        \"assistant\": \"blue\",\n",
        "        \"tool\": \"magenta\",\n",
        "    }\n",
        "    for message in messages:\n",
        "        print(\n",
        "            colored(\n",
        "                f\"{message['role']}: {message['content']}\\n\\n\",\n",
        "                role_to_color[message[\"role\"]],\n",
        "            )\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TVKTIT8fOaGH",
        "outputId": "41c56b41-0637-4ea9-b26e-446265583e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Agent conversation"
      ],
      "metadata": {
        "id": "uPRSxD-LYXZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with a system message\n",
        "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
        "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
        "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
        "Begin!\"\"\"\n",
        "messages = [{\"role\": \"system\", \"content\": paper_system_message}]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jM1n397BYaqn",
        "outputId": "4917c472-3e1b-426f-d1fd-ad64a9dc31e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a user message\n",
        "messages.append({\"role\": \"user\", \"content\": \"Hi, how does PPO reinforcement learning work?\"})\n",
        "messages = run_conversation(\n",
        "    messages, tools=arxiv_tools\n",
        ")\n",
        "display_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "aEDnFVtDYhgz",
        "outputId": "0044a5bf-9852-4ff3-b92c-041145af8111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting search results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-99-36bb5578836d>:17: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got tool results, asking model to continue.\n",
            "system: You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
            "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
            "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
            "Begin!\n",
            "\n",
            "\n",
            "user: Hi, how does PPO reinforcement learning work?\n",
            "\n",
            "\n",
            "assistant: \n",
            "\n",
            "\n",
            "tool: Articles added to knowledge base; can now call `read_article_and_summarize`\n",
            "\n",
            "\n",
            "assistant: I found a paper that explains how Proximal Policy Optimization (PPO) reinforcement learning works. The paper is titled \"Proximal Policy Optimization Algorithms\" by John Schulman et al. It provides a clear explanation of the PPO algorithm and its advantages in reinforcement learning.\n",
            "\n",
            "Here is the link to the paper: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
            "\n",
            "Shall I provide a summary of the paper for you?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a user message\n",
        "messages.append({\"role\": \"user\", \"content\": \"Yes please do.\"})\n",
        "messages = run_conversation(\n",
        "    messages, tools=arxiv_tools\n",
        ")\n",
        "display_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "ldDB5kAxjs1I",
        "outputId": "ebc2c0eb-435c-4778-dcfc-62d7553dd0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding and reading paper\n",
            "Chunking text from paper\n",
            "Summarizing each chunk of text\n",
            "Summarizing into overall summary\n",
            "Got tool results, asking model to continue.\n",
            "system: You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
            "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
            "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
            "Begin!\n",
            "\n",
            "\n",
            "user: Hi, how does PPO reinforcement learning work?\n",
            "\n",
            "\n",
            "assistant: \n",
            "\n",
            "\n",
            "tool: Articles added to knowledge base; can now call `read_article_and_summarize`\n",
            "\n",
            "\n",
            "assistant: I found a paper that explains how Proximal Policy Optimization (PPO) reinforcement learning works. The paper is titled \"Proximal Policy Optimization Algorithms\" by John Schulman et al. It provides a clear explanation of the PPO algorithm and its advantages in reinforcement learning.\n",
            "\n",
            "Here is the link to the paper: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
            "\n",
            "Shall I provide a summary of the paper for you?\n",
            "\n",
            "\n",
            "user: Yes please do.\n",
            "\n",
            "\n",
            "assistant: \n",
            "\n",
            "\n",
            "tool: Core Argument:\n",
            "The paper discusses the Proximal Policy Optimization (PPO) algorithm in deep reinforcement learning and its performance in continuous control tasks. It proposes an exploration enhancement mechanism called Intrinsic Exploration Module PPO (IEM-PPO) to address the problem of exploration in complex environments.\n",
            "\n",
            "Evidence:\n",
            "The paper evaluates the IEM-PPO algorithm on multiple tasks of the MuJoCo physical simulator and compares it with other algorithms. It discusses the limitations of current exploration mechanisms and proposes a new mechanism based on uncertainty theory. The algorithm is tested on various simulation tasks and compared with other curiosity-driven and intrinsic exploration module algorithms.\n",
            "\n",
            "Conclusions:\n",
            "The IEM-PPO algorithm outperforms other algorithms in terms of sample efficiency and cumulative reward, and demonstrates stability and robustness. It balances the role ratio of uncertainty reward and external environment reward in training. The paper suggests future work in discrete action tasks, internal incentive consistency, and multi-agent learning.\n",
            "\n",
            "\n",
            "assistant: The paper \"Proximal Policy Optimization Algorithms\" by John Schulman et al. discusses the Proximal Policy Optimization (PPO) algorithm in deep reinforcement learning and its performance in continuous control tasks. It introduces an exploration enhancement mechanism called Intrinsic Exploration Module PPO (IEM-PPO) to address the problem of exploration in complex environments. The paper evaluates the IEM-PPO algorithm on multiple tasks of the MuJoCo physical simulator and compares it with other algorithms. The algorithm outperforms other algorithms in terms of sample efficiency and cumulative reward, demonstrating stability and robustness. If you would like to delve deeper into the details of PPO reinforcement learning and its exploration enhancement mechanism, you can access the paper using the following link: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0 Named entity recognition\n",
        "If time allows, we can take a look at\n",
        "https://cookbook.openai.com/examples/named_entity_recognition_to_enrich_text or any other usecase participants are interested in.\n"
      ],
      "metadata": {
        "id": "ZisB7Xsz5A3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "- [OpenAI Quickstart](https://platform.openai.com/docs/quickstart?context=python)\n",
        "- [OpenAI Cookbook](https://cookbook.openai.com/)"
      ],
      "metadata": {
        "id": "K8G26G4u2AgM"
      }
    }
  ]
}